{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import importlib\n",
    "import scipy.optimize as opt\n",
    "import dlm\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report,f1_score, precision_score, recall_score, accuracy_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_by_nomr(df):\n",
    "    \n",
    "    Y_train = []\n",
    "\n",
    "    \n",
    "    labels_Y = []\n",
    "  \n",
    "    for intensity in range(3):\n",
    "\n",
    "        Y_data = df[intensity]\n",
    "\n",
    "\n",
    "        Y_data[\"3D_acc_norm\"] = np.sqrt(Y_data[4]**2 + Y_data[5]**2 + Y_data[6]**2)\n",
    "        Y_data[\"3D_gyr_norm\"] = np.sqrt(Y_data[10]**2 + Y_data[11]**2 + Y_data[12]**2)\n",
    "\n",
    "        Y_train.append(Y_data[[\"3D_acc_norm\",\"3D_gyr_norm\"]].iloc[:])\n",
    "\n",
    "    \n",
    "        labels_Y.append([intensity]*len(Y_train[intensity]))\n",
    "    \n",
    "    \n",
    "        \n",
    "    labels_Y = np.concatenate(labels_Y)\n",
    "    \n",
    "    return Y_train, labels_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_intense_acts = [1,2,3,4,17]\n",
    "\n",
    "middle_intense_acts = [13,16,7] \n",
    "\n",
    "high_intense_acts = [5,6,24,12]\n",
    "\n",
    "def report_results(subjectID):\n",
    "\n",
    "    df_train = pd.read_csv(f\"subject10{subjectID}.dat\",sep=\" \",header=None)\n",
    "    df_train = df_train.interpolate()\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    df_low_intense_acts = df_train[df_train[1].isin(low_intense_acts)]\n",
    "    df_middle_intense_acts = df_train[df_train[1].isin(middle_intense_acts)]\n",
    "    df_high_intense_acts = df_train[df_train[1].isin(high_intense_acts)]\n",
    "\n",
    "    df_train = [df_low_intense_acts, df_middle_intense_acts, df_high_intense_acts]\n",
    "    \n",
    "    Y_train,labels_Y_train = preprocess_by_nomr(df_train)\n",
    "\n",
    "    bdlm = dlm.ensemble_BDLM(n_acts = 3)\n",
    "    \n",
    "    bdlm.fit(Y_train)\n",
    "\n",
    "    \n",
    "    user_stats = {}\n",
    "\n",
    "    user_stats[\"subjectID\"] = f\"subject{subjectID}\"\n",
    "    \n",
    "    user_stats[\"labels_Y_train\"] = labels_Y_train\n",
    "    \n",
    "    user_stats[\"Y_train\"] = Y_train\n",
    "    \n",
    "    user_stats[\"bdlm\"] = bdlm \n",
    " \n",
    "    \n",
    "    return user_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training subject 1\n",
      "Finished Training subject 2\n",
      "Finished Training subject 3\n",
      "Finished Training subject 4\n",
      "Finished Training subject 5\n",
      "Finished Training subject 6\n",
      "Finished Training subject 7\n",
      "Finished Training subject 8\n"
     ]
    }
   ],
   "source": [
    "subjects_report = []\n",
    "\n",
    "for i in range(1,9):\n",
    "    \n",
    "    stats = report_results(subjectID = i)\n",
    "    subjects_report.append(stats)\n",
    "    \n",
    "    print(f\"Finished Training subject {i}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0\n",
      "Subject 1\n",
      "Subject 2\n",
      "Subject 3\n",
      "Subject 4\n",
      "Subject 5\n",
      "Subject 6\n",
      "Subject 7\n"
     ]
    }
   ],
   "source": [
    "macro_precision = []\n",
    "macro_recall = []\n",
    "accuracy = []\n",
    "\n",
    "for j in range(len(subjects_report)):\n",
    "\n",
    "    Y_subject = subjects_report[j][\"Y_train\"]\n",
    "    y_test = subjects_report[j][\"labels_Y_train\"]\n",
    "    subjectID = subjects_report[j][\"subjectID\"]\n",
    "\n",
    "    voting_p = []\n",
    "    \n",
    "    for i in range(len(subjects_report)):\n",
    "\n",
    "        if subjects_report[i][\"subjectID\"] == subjectID:\n",
    "\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            probs = []\n",
    "            for activity in Y_subject:\n",
    "\n",
    "                _, p = subjects_report[i][\"bdlm\"].predict(activity, return_probs=True,restart=True)\n",
    "                del _\n",
    "                probs.append(p)\n",
    "\n",
    "        voting_p.append(probs)\n",
    "\n",
    "    p = np.zeros(np.concatenate(voting_p[0]).shape)\n",
    "    \n",
    "    for probability in voting_p:\n",
    "        p += np.concatenate(probability)\n",
    "\n",
    "    N = len(voting_p)\n",
    "    p = p/N\n",
    "\n",
    "    y_hat = np.argmax(p, axis=1)\n",
    "\n",
    "    precision = precision_score(y_test,y_hat,average = \"macro\")\n",
    "    recall = recall_score(y_test,y_hat,average = \"macro\")\n",
    "    accuracy_ = accuracy_score(y_test,y_hat)\n",
    "\n",
    "    macro_precision.append(precision)\n",
    "    macro_recall.append(recall)\n",
    "    accuracy.append(accuracy_)\n",
    "\n",
    "    print(f\"Subject {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRECISION': [0.526, 0.059],\n",
       " 'RECALL': [0.4754, 0.0487],\n",
       " 'ACCURACY': [0.4952, 0.0319]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_precision = np.array(macro_precision)    \n",
    "macro_recall = np.array(macro_recall)    \n",
    "accuracy = np.array(accuracy)  \n",
    "\n",
    "model_report = {}\n",
    "\n",
    "model_report[\"PRECISION\"] = [round(macro_precision.mean(), 4),round(macro_precision.std(), 4)]\n",
    "model_report[\"RECALL\"] = [round(macro_recall.mean(), 4),round(macro_recall.std(), 4)]\n",
    "model_report[\"ACCURACY\"] = [round(accuracy.mean(), 4),round(accuracy.std(), 4)]\n",
    "model_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reports = pd.DataFrame({\n",
    "    'Subject': [f'subject{i}' for i in range(1, 9)],\n",
    "    'Accuracy': accuracy,\n",
    "    'Macro Precision': macro_precision,\n",
    "    'Macro Recall': macro_recall\n",
    "})\n",
    "\n",
    "df_reports.to_csv(\"cross_user_BDLM.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53945605, 0.59173627, 0.58025843, 0.61454315, 0.47794626,\n",
       "       0.46976915, 0.45871484, 0.47523853])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

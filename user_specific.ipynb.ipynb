{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dlm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report,f1_score, precision_score, recall_score, accuracy_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lying', 'sitting', 'standing', 'walking', 'ironing']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities_names = {1:\"lying\", 2:\"sitting\", 3:\"standing\", 4:\"walking\", 5:\"running\", 6:\"cycling\", 7:\"Nordic walking\",\n",
    "9:\"watching TV\", 10:\"computer work\", 11:\"car driving\", 12:\"ascending stairs\", 13:\"descending stairs\",\n",
    "16:\"vacuum cleaning\", 17:\"ironing\", 18:\"folding laundry\", 19:\"house cleaning\", 20:\"playing soccer\", 24:\"rope jumping\"}\n",
    "\n",
    "low_intense_acts = [1,2,3,4,17]\n",
    "\n",
    "middle_intense_acts = [13,16,7] \n",
    "\n",
    "high_intense_acts = [5,6,24,12]\n",
    "\n",
    "[activities_names[act] for act in low_intense_acts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['descending stairs', 'vacuum cleaning', 'Nordic walking']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[activities_names[act] for act in middle_intense_acts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['running', 'cycling', 'rope jumping', 'ascending stairs']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[activities_names[act] for act in high_intense_acts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregate_similar_acts(df,acts, intensity):\n",
    "\n",
    "\n",
    "    def preprocess_by_nomr(Y_data):\n",
    "\n",
    "    \n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "        \n",
    "        \n",
    "        N = len(Y_data)\n",
    "\n",
    "        Y_data[\"3D_acc_norm\"] = np.sqrt(Y_data[4]**2 + Y_data[5]**2 + Y_data[6]**2)\n",
    "        Y_data[\"3D_gyr_norm\"] = np.sqrt(Y_data[10]**2 + Y_data[11]**2 + Y_data[12]**2)\n",
    "       # Y_data[\"3D_mag_norm\"] = np.sqrt(Y_data[13]**2 + Y_data[14]**2 + Y_data[15]**2)\n",
    "\n",
    "        Y_train = Y_data[[\"3D_acc_norm\",\"3D_gyr_norm\"]].iloc[:int(N/2)]\n",
    "        Y_test = Y_data[[\"3D_acc_norm\",\"3D_gyr_norm\"]].iloc[int(N/2):]\n",
    "        \n",
    "        \n",
    "        labels_Y_lrain = [intensity]*len(Y_train)\n",
    "        labels_Y_lest = [intensity]*len(Y_test)\n",
    "        \n",
    "        \n",
    "        return Y_train,Y_test,labels_Y_lrain,labels_Y_lest\n",
    "\n",
    "\n",
    "\n",
    "    df_train = []\n",
    "    df_test = []\n",
    "\n",
    "    label_df_train = []\n",
    "    label_df_test = []\n",
    "\n",
    "\n",
    "    for act in acts:\n",
    "\n",
    "        df_aux = df[df[1] == act]\n",
    "\n",
    "        if len(df_aux) > 0:\n",
    "\n",
    "            Y_train,Y_test,labels_Y_train,labels_Y_lest = preprocess_by_nomr(df_aux)\n",
    "\n",
    "\n",
    "            df_train.append(Y_train)\n",
    "            df_test.append(Y_test)\n",
    "\n",
    "\n",
    "            label_df_train.append(labels_Y_train)\n",
    "            label_df_test.append(labels_Y_lest)\n",
    "\n",
    "    label_df_train = np.concatenate(label_df_train)\n",
    "    labels_df_lest = np.concatenate(label_df_test)\n",
    "\n",
    "    return pd.concat(df_train), label_df_train, pd.concat(df_test), labels_df_lest\n",
    "\n",
    "\n",
    "\n",
    "def train_test_agregated(number,acts, intensity):\n",
    "\n",
    "    df = pd.read_csv(f\"subject10{number}.dat\",sep=\" \",header=None)\n",
    "    df = df.interpolate()\n",
    "\n",
    "\n",
    "    df_acts = df[df[1].isin(acts)]\n",
    "\n",
    "    X_train, y_train, X_test, y_test = agregate_similar_acts(df_acts,acts,intensity)\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def MET_activities_aggregation(userID, merge = False):\n",
    "\n",
    "\n",
    "    low_intense_acts = [1,2,3,4,17]\n",
    "\n",
    "    middle_intense_acts = [13,16,7] \n",
    "\n",
    "    high_intense_acts = [5,6,24,12]\n",
    "\n",
    "    all_acts = [low_intense_acts, middle_intense_acts, high_intense_acts]\n",
    "\n",
    "    df_train = []\n",
    "    label_train= []\n",
    "    df_test= []\n",
    "    label_test = []\n",
    "\n",
    "\n",
    "    for j in range(len(all_acts)):\n",
    "\n",
    "        X_train, y_train, X_test, y_test = train_test_agregated(userID, all_acts[j],j)\n",
    "\n",
    "\n",
    "        df_train.append(X_train)\n",
    "        label_train.append(y_train)\n",
    "        df_test.append(X_test)\n",
    "        label_test.append(y_test)\n",
    "\n",
    "    label_train= np.concatenate(label_train)\n",
    "\n",
    "    label_test = np.concatenate(label_test)\n",
    "    \n",
    "    if merge is True:\n",
    "\n",
    "        df_train = pd.concat(df_train)\n",
    "        \n",
    "        df_test= pd.concat(df_test)\n",
    "        \n",
    "\n",
    "    return df_train ,label_train,df_test,label_test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(subjectID):\n",
    "\n",
    "\n",
    "    df_train , _ ,df_test,label_test = MET_activities_aggregation(subjectID, merge = False)\n",
    "\n",
    "    bdlm = dlm.ensemble_BDLM(n_acts = 3)\n",
    "    bdlm.fit(df_train)\n",
    "\n",
    "    y_hat = []\n",
    "    \n",
    "    for act in range(3):\n",
    "\n",
    "        y_hat.append(bdlm.predict(df_test[act], return_probs=False,restart=False))\n",
    "\n",
    "    y_hat = np.concatenate(y_hat)\n",
    "\n",
    "    params_stats = {}\n",
    "\n",
    "    precision = precision_score(label_test,y_hat,average = \"macro\")\n",
    "    recall = recall_score(label_test,y_hat,average = \"macro\")\n",
    "    accuracy = accuracy_score(label_test,y_hat)\n",
    " \n",
    "    params_stats[\"macro_precision\"] = precision\n",
    "    params_stats[\"macro_recall\"] = recall\n",
    "    params_stats[\"accuracy\"] = accuracy\n",
    "    \n",
    "    return params_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training subject 1\n",
      "Finished Training subject 2\n",
      "Finished Training subject 3\n",
      "Finished Training subject 4\n",
      "Finished Training subject 5\n",
      "Finished Training subject 6\n",
      "Finished Training subject 7\n",
      "Finished Training subject 8\n"
     ]
    }
   ],
   "source": [
    "subjects_report = []\n",
    "\n",
    "for i in [1,2,3,4,5,6,7,8]:\n",
    "\n",
    "    stats = report_results(subjectID = i)\n",
    "    subjects_report.append(stats)\n",
    "    \n",
    "    print(f\"Finished Training subject {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRECISION': [0.5725, 0.0897],\n",
       " 'RECALL': [0.5453, 0.1216],\n",
       " 'ACCURACY': [0.5473, 0.0692]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_report = {}\n",
    "\n",
    "\n",
    "macro_precision = []\n",
    "macro_recall = []\n",
    "accuracy = []\n",
    "\n",
    "for report in subjects_report:\n",
    "    \n",
    "    macro_precision.append(report[\"macro_precision\"])\n",
    "    macro_recall.append(report[\"macro_recall\"])\n",
    "    accuracy.append(report[\"accuracy\"])\n",
    "\n",
    "macro_precision = np.array(macro_precision)    \n",
    "macro_recall = np.array(macro_recall)    \n",
    "accuracy = np.array(accuracy) \n",
    "\n",
    "model_report[\"PRECISION\"] = [round(macro_precision.mean(), 4),round(macro_precision.std(), 4)]\n",
    "model_report[\"RECALL\"] = [round(macro_recall.mean(), 4),round(macro_recall.std(), 4)]\n",
    "model_report[\"ACCURACY\"] = [round(accuracy.mean(), 4),round(accuracy.std(), 4)]\n",
    "\n",
    "model_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reports = pd.DataFrame({\n",
    "    'Subject': [f'subject{i}' for i in range(1, 9)],\n",
    "    'Accuracy': accuracy,\n",
    "    'Macro Precision': macro_precision,\n",
    "    'Macro Recall': macro_recall\n",
    "})\n",
    "\n",
    "df_reports.to_csv(\"user_dependent_pamap2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_baseline(model):\n",
    "    \n",
    "    \n",
    "\n",
    "    macro_precision = []\n",
    "    macro_recall = []\n",
    "    accuracy = []\n",
    "\n",
    "    for i in [1,2,3,4,5,6,7,8]: \n",
    "\n",
    "        \n",
    "        X_train,y_train ,X_test,y_test = MET_activities_aggregation(i, merge = True)\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "        precision = precision_score(y_test,y_hat,average = \"macro\")\n",
    "        recall = recall_score(y_test,y_hat,average = \"macro\")\n",
    "        accuracy_ = accuracy_score(y_test,y_hat)\n",
    "\n",
    "        macro_precision.append(precision)\n",
    "        macro_recall.append(recall)\n",
    "        accuracy.append(accuracy_)\n",
    "\n",
    "    macro_precision = np.array(macro_precision)    \n",
    "    macro_recall = np.array(macro_recall)    \n",
    "    accuracy = np.array(accuracy)  \n",
    "\n",
    "    model_report = {}\n",
    "\n",
    "    model_report[\"PRECISION\"] = [round(macro_precision.mean(), 4),round(macro_precision.std(), 4)]\n",
    "    model_report[\"RECALL\"] = [round(macro_recall.mean(), 4),round(macro_recall.std(), 4)]\n",
    "    model_report[\"ACCURACY\"] = [round(accuracy.mean(), 4),round(accuracy.std(), 4)]\n",
    "    \n",
    "    \n",
    "    return model_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished to GaussianNB()\n",
      "{'PRECISION': [0.5522, 0.0478], 'RECALL': [0.4517, 0.0322], 'ACCURACY': [0.6036, 0.0501]}\n",
      "training finished to KNeighborsClassifier()\n",
      "{'PRECISION': [0.5441, 0.0181], 'RECALL': [0.505, 0.0241], 'ACCURACY': [0.6178, 0.0511]}\n",
      "training finished to RandomForestClassifier(n_jobs=-1)\n",
      "{'PRECISION': [0.5364, 0.0142], 'RECALL': [0.5154, 0.0246], 'ACCURACY': [0.615, 0.0507]}\n",
      "training finished to LogisticRegression(n_jobs=-1)\n",
      "{'PRECISION': [0.4209, 0.0917], 'RECALL': [0.39, 0.0389], 'ACCURACY': [0.5656, 0.0678]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "baselines = [GaussianNB(),KNN(),RFC(n_jobs=-1),LogisticRegression(n_jobs=-1)]\n",
    "\n",
    "report_baselines = {}\n",
    "\n",
    "for baseline in baselines:\n",
    "    \n",
    "    model = baseline\n",
    "    \n",
    "    result = fit_baseline(model)\n",
    "    \n",
    "    report_baselines[f\"{baseline}\".strip(\"()\")] = [result]\n",
    "    \n",
    "    print(f\"training finished to {baseline}\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_precision = []\n",
    "means_recall = []\n",
    "means_accuracy = []\n",
    "\n",
    "stds_precision = []\n",
    "stds_recall = []\n",
    "stds_accuracy = []\n",
    "\n",
    "\n",
    "baselines = list(report_baselines.keys())\n",
    "for i in range(5):\n",
    "    \n",
    "    if i == 0:\n",
    "        \n",
    "        means_precision.append(model_report['PRECISION'][0])\n",
    "        means_recall.append(model_report['RECALL'][0])\n",
    "        means_accuracy.append(model_report['ACCURACY'][0])\n",
    "\n",
    "        stds_precision.append(model_report['PRECISION'][1])\n",
    "        stds_recall.append(model_report['RECALL'][1])\n",
    "        stds_accuracy.append(model_report['ACCURACY'][1])\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        model_report = report_baselines[baselines[i-1]][0]\n",
    "        \n",
    "        means_precision.append(model_report['PRECISION'][0])\n",
    "        means_recall.append(model_report['RECALL'][0])\n",
    "        means_accuracy.append(model_report['ACCURACY'][0])\n",
    "\n",
    "        stds_precision.append(model_report['PRECISION'][1])\n",
    "        stds_recall.append(model_report['RECALL'][1])\n",
    "        stds_accuracy.append(model_report['ACCURACY'][1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"model\": [\"BDLM\"] + baselines, \"means_precision\":means_precision, \"stds_precision\":stds_precision,\n",
    "                  \"means_recall\":means_recall, \"stds_recall\":stds_recall,\"means_accuracy\": means_accuracy, \"stds_accuracy\":stds_accuracy})\n",
    "df.to_csv(\"user_specific_pamap2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
